library(ape)
library(vegan)
library(ggplot2)
library(picante)
library(plyr)
library(dada2); packageVersion("dada2")

#setwd('~/Dropbox (Smithsonian)/CBZooplankton_Metabarcode_Results/')
#Following tutorial for DADA2 at: https://benjjneb.github.io/dada2/tutorial.html
#On second pass followed tutorial for ITS1 because it contains info to remove primers:
#https://benjjneb.github.io/dada2/ITS_workflow.html
list.files(getwd())
path <- "~/Dropbox (Smithsonian)/MERC_18S_Fastq_Sequences/Raw_seqs/" #directory with the fastq files after unzipping
list.files(path)

# Forward and reverse fastq filenames have format: SAMPLENAME_R1_001.fastq and SAMPLENAME_R2_001.fastq
fnFs <- sort(list.files(path, pattern="_R1_001.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_001.fastq", full.names = TRUE))

# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)

#generate quality plots of 2 samples in forward and reverse -- can see primers present
plotQualityProfile(fnFs[1:2], n = 10000) 
plotQualityProfile(fnRs[1:2]) 

#Primers removed with cutadapt, run separately in terminal with BratePrimers_cutadapt.txt
#Output from cutadapt saved to "Results_cutadapt_XXX"
path2 = "~/Dropbox (Smithsonian)/XXXX/trimmed/"
list.files(path2)

#Read the trimmed sequence files in
cutFs <- sort(list.files(path2, pattern = "_R1_001_trim.fastq", full.names = TRUE))
cutRs <- sort(list.files(path2, pattern = "_R2_001_trim.fastq", full.names = TRUE))

get.sample.name <- function(fname) strsplit(basename(fname), "_")[[1]][1]
sample.names <- unname(sapply(cutFs, get.sample.name))
head(sample.names)

#Check the quality of reads in the sequences with the primers removed -- cobfirm primers are gone
plotQualityProfile(cutFs[1:2])
plotQualityProfile(cutRs[1:2])

# Place primer trimmed, filtered files in filtered/ subdirectory, to be stored as fastq.gz files
filtFs <- file.path(path2, "filtered", basename(cutFs))
filtRs <- file.path(path2, "filtered", basename(cutRs))

#Removing spurious small sequences and phix, try various outcomes for this to see how to save more sequences by making this step more lenient
#out = standard filtering parameters by dada2
#out3 = same as out4, but maxEE = (3,3)
out4 <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, maxN = 0, maxEE = c(4, 4), 
                     truncQ = 2, minLen = 50, rm.phix = TRUE, compress = TRUE, multithread = TRUE)

# The DADA2 algorithm makes use of a parametric error model (err) and every amplicon dataset has a different set of error rates. The learnErrors method learns this error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. As in many machine-learning problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors).

errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)

#To explore the errors in base switching
plotErrors(errF, nominalQ=TRUE)

# Explanation of plot: The error rates for each possible transition (A→C, A→G, …) are shown. 
# Points are the observed error rates for each consensus quality score. 
# The black line shows the estimated error rates after convergence of the machine-learning algorithm. 
# The red line shows the error rates expected under the nominal definition of the Q-score. 
# Here the estimated error rates (black line) are a good fit to the observed rates (points), 
# and the error rates drop with increased quality as expected. Everything looks reasonable.

#Dereplicate identical reads
derepFs <- derepFastq(filtFs, verbose = TRUE)
derepRs <- derepFastq(filtRs, verbose = TRUE)
# Name the derep-class objects by the sample names
names(derepFs) <- sample.names
names(derepRs) <- sample.names

#Sample inference- look at core sample inference algorithm described in paper
dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
dadaRs <- dada(derepRs, err=errR, multithread=TRUE)

#To look at returned dada-class object:
dadaFs[[12]]#Make sure you are looking at a sample and not a negative control for this check
dadaRs[[12]]

#Merge the sequences
# By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, 
# and are identical to each other in the overlap region (but these conditions can be changed via function arguments).
#As the overlap region for these should be large (>50 bp), I'm going to allow for a lot more mismatches
#as I would have if I were doing this in usearch.
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE, maxMismatch = 10)

# Inspect the merger data.frame from the first sample
head(mergers[[1]])#Make sure you don't do this check with a negative control

#To make a ASV table
MERCseqtab <- makeSequenceTable(mergers)
dim(MERCseqtab) 

# Inspect distribution of sequence lengths
table(nchar(getSequences(MERCseqtab)))

#A number of these sequences are WAY too small or large to be correct, so I'm going to trim them out.
#Keeping sequences between 410 and 440 bp, based on output
MERCseqtab2 <- MERCseqtab[,nchar(colnames(MERCseqtab)) %in% 400:440]
dim(MERCseqtab2)
table(nchar(getSequences(MERCseqtab2)))

#Remove chimeras
MERCseqtab.nochim <- removeBimeraDenovo(MERCseqtab2, method="consensus", multithread=TRUE, verbose=TRUE)


dim(MERCseqtab.nochim)


sum(MERCseqtab.nochim)/sum(MERCseqtab2)

#Final check allows you to track reads through the pipeline
getN <- function(x) sum(getUniques(x))
track <- cbind(out4, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(MERCseqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("reads.in", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
track
#results of track saved in excel document
write.csv(track, "~/Desktop/MERC18S_track.csv")

#I want to keep this as an output table
write.csv(MERCseqtab.nochim, "~/Desktop/MERC18S_ASVtable.csv")

#Testing DADA2 assign taxonomy, downloaded the recommended pr2 database for this purpose and put into a new folder
#the command "AssignTaxonomy" was run on the console because it took a lot of RAM to run -- took about 5 hours to complete
#Now need to import the taxa table for use.
#This was the command used for the taxonomic assignment.
#taxa <- assignTaxonomy(MERCseqtab.nochim, "~/R_directory/pr2_version_4.12.0_18S_dada2.fasta", multithread=TRUE)
write.csv(taxa, "~/Desktop/MERC18S_taxa.csv")

#At the end of this pipeline, I had separate otu, taxa, and sample_metadata files, which I could then load separately to use the phyloseq pipeline to explore each dataset.
